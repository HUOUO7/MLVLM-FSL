import pandas as pd

def calculate_top_k_accuracy(all_avg_indices, ground_truths, k):
    correct_count = 0
    for indices, truth in zip(all_avg_indices, ground_truths):
        all_avg_index = list(map(int, indices.split('-')))
        # Check that ground_truth is in the top-k index
        if truth in all_avg_index[:k]:
            correct_count += 1

    accuracy = correct_count / len(ground_truths)
    return accuracy
# step 1
# Given Datasetname
# Given file_dir that generated by 1_cal_aggregate_similarity.py
datasetname ="CUB"
file_dir = f"file_dir that generated by 1_cal_aggregate_similarity.py"
df = pd.read_csv(file_dir)

# Automatically extract relevant fields (header row)
fields = df.columns.tolist()

# step 2
# Calculate the accuracy of each field
correct_rates = {}
for field in fields:

    if field != 'id' and field != 'ground_idx' and field != 'cartegories' and field != 'all_avg_index':
        correct_count = (df[field] == df['ground_idx']).sum()
        print(correct_count)
        total_count = len(df)
        accuracy = correct_count / total_count
        correct_rates[field] = accuracy
    correct_count = (df[field] == df['ground_idx']).sum()
    total_count = len(df)
    accuracy = correct_count / total_count
    correct_rates[field] = accuracy

# Step 3
# Print the accuracy of each field
for field, accuracy in correct_rates.items():
    print(f"{field} accuracy: {accuracy:.2%}")

all_avg_indices = df['all_avg_index'].tolist()
ground_truths = df['ground_idx'].tolist()

top_k_accuracies = {}
for k in range(1, 6):
    accuracy = calculate_top_k_accuracy(all_avg_indices, ground_truths, k)
    top_k_accuracies[f'Top-{k} Accuracy'] = accuracy

# Print the accuracy of top-k
for key, value in top_k_accuracies.items():
    print(f"{key}: {value:.2%}")
